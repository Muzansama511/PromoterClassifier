{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "from Bio.SeqIO import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 82, 4])\n",
      "torch.Size([1000, 4, 82])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./database/test.fa', \"rt\") as handle:\n",
    "    database = []\n",
    "    for record in parse(handle, \"fasta\"):\n",
    "        # print(type(record))\n",
    "        database.append(record.seq)\n",
    "\n",
    "dictionary = ['A','T','G','C']\n",
    "dict_for_cmap = ['PAD','A','T','G','C']\n",
    "\n",
    "len_cutoff = 80\n",
    "\n",
    "\n",
    "sample_I = np.eye(len(dictionary))\n",
    "tokenized = np.array([np.array([np.zeros(len(dictionary))]+[sample_I[dictionary.index(i)] for i in database[j][:min(len(database[j]),len_cutoff)]]+[np.zeros(len(dictionary))]*(len_cutoff-len(database[j])+1)) for j in range((len(database)))])\n",
    "test_data = torch.tensor(tokenized,dtype=float).to('cuda')\n",
    "test_data_CNN = torch.permute(test_data, (0,2,1)).to('cuda')\n",
    "print(test_data.shape)\n",
    "print(test_data_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self,input_dims=4,hidden_dims=1,num_output_neurons=1,num_layers = 1,bidirectionality = False):\n",
    "        super().__init__()\n",
    "        self.inp = input_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.hid = hidden_dims\n",
    "        self.out = num_output_neurons\n",
    "        self.bidir = bidirectionality\n",
    "        self.recording = True\n",
    "        self.record = {}\n",
    "        D = 4 if self.bidir else 1\n",
    "        self.RNN = nn.LSTM(self.inp,self.hid,self.num_layers,batch_first = True,bidirectional=self.bidir)\n",
    "        # self.DNN1 = nn.Linear(D*self.hid,D*self.hid)\n",
    "        # self.bn1 = nn.BatchNorm1d(D*self.hid)\n",
    "        # self.DNN2 = nn.Linear(D*self.hid,D*self.hid)\n",
    "        # self.bn2 = nn.BatchNorm1d(D*self.hid)\n",
    "        # self.DNN3 = nn.Linear(D*self.hid,D*self.hid)\n",
    "        # self.bn3 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN4 = nn.Linear(D*self.hid,self.out)\n",
    "    def start_recording(self):\n",
    "        self.recording = True\n",
    "    def stop_recording(self):\n",
    "        self.recording = False\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Note: x should be ending with a $ sign, the encoding of which is [0,0,0,0]. This will be passed to the DNN\n",
    "        if is bidirectional, then should start and end with $ signs\n",
    "        '''\n",
    "\n",
    "        out,_ = self.RNN(x)\n",
    "        # print(out,type(out))\n",
    "        if not self.bidir:\n",
    "            last_token = out[:,-1,:]\n",
    "            # print(last_token.shape)\n",
    "            # out = F.dropout(F.relu(self.bn1(self.DNN1(last_token))),p=0.1)\n",
    "            # out = F.dropout(F.relu(self.bn2(self.DNN2(out))),p=0.1)\n",
    "            # out = F.dropout(F.relu(self.bn3(self.DNN3(out))),p=0.1)\n",
    "            # out = (self.DNN4(out))\n",
    "            out = (self.DNN4(last_token))\n",
    "            # print(out.shape)\n",
    "            if self.recording:\n",
    "                self.record = {'last_token':last_token,'out1':out}\n",
    "        else:\n",
    "            # NOTE: This does not function correctly DO NOT USE\n",
    "            last_token = out[:,-1,:]\n",
    "            first_token = out[:,0,:]\n",
    "            # out = F.dropout(F.relu(self.bn1(self.DNN1(torch.concatenate((first_token,last_token),dims=-1)))),p=0.1)\n",
    "            # out = F.dropout(F.relu(self.bn2(self.DNN2(out))),p=0.1)\n",
    "            # out = F.dropout(F.relu(self.bn3(self.DNN3(out))),p=0.1)\n",
    "            out = (self.DNN4(out))\n",
    "        out = F.sigmoid(out)\n",
    "        # print(out.shape)\n",
    "        if self.recording:\n",
    "            self.record['out'] = out\n",
    "\n",
    "        return torch.reshape(out,(-1,self.out))\n",
    "\n",
    "\n",
    "class LSTMModelWithSkip(nn.Module):\n",
    "    def __init__(self,input_dims=4,hidden_dims=64,num_output_neurons=1,num_layers = 1,bidirectionality = False):\n",
    "        super().__init__()\n",
    "        self.inp = input_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.hid = hidden_dims\n",
    "        self.out = num_output_neurons\n",
    "        self.bidir = bidirectionality\n",
    "        self.recording = True\n",
    "        self.record={}\n",
    "        D = 4 if self.bidir else 1\n",
    "        self.RNN = nn.LSTM(self.inp,self.hid,self.num_layers,batch_first = True,bidirectional=self.bidir)\n",
    "\n",
    "        self.DNN1 = nn.Linear(D*self.hid,D*self.hid)\n",
    "        self.bn1 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN2 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn2 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN3 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn3 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN4 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn4 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN5 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn5 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN6 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn6 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN7 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn7 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN8 = nn.Linear(2*D*self.hid,D*self.hid)\n",
    "        self.bn8 = nn.BatchNorm1d(D*self.hid)\n",
    "        self.DNN9 = nn.Linear(2*D*self.hid,self.out)\n",
    "    def start_recording(self):\n",
    "        self.recording = True\n",
    "    def stop_recording(self):\n",
    "        self.recording = False\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        Note: x should be ending with a $ sign, the encoding of which is [0,0,0,0]. This will be passed to the DNN\n",
    "        if is bidirectional, then should start and end with $ signs\n",
    "        '''\n",
    "\n",
    "        out,_ = self.RNN(x)\n",
    "        # print(out,type(out))\n",
    "        if not self.bidir:\n",
    "            last_token = out[:,-1,:]\n",
    "            out1 = F.dropout(F.relu(self.bn1(self.DNN1(last_token))),p=0.1)\n",
    "            out2 = F.dropout(F.relu(self.bn2(self.DNN2(torch.concatenate((last_token,out1),dim=-1)))),p=0.1)\n",
    "            out3 = F.dropout(F.relu(self.bn3(self.DNN3(torch.concatenate((out1,out2),dim=-1)))),p=0.1)\n",
    "            out4 = F.dropout(F.relu(self.bn4(self.DNN4(torch.concatenate((out2,out3),dim=-1)))),p=0.1)\n",
    "            out5 = F.dropout(F.relu(self.bn3(self.DNN5(torch.concatenate((out3,out4),dim=-1)))),p=0.1)\n",
    "            out6 = F.dropout(F.relu(self.bn4(self.DNN6(torch.concatenate((out4,out5),dim=-1)))),p=0.1)\n",
    "            out7 = F.dropout(F.relu(self.bn3(self.DNN7(torch.concatenate((out5,out6),dim=-1)))),p=0.1)\n",
    "            out8 = F.dropout(F.relu(self.bn4(self.DNN8(torch.concatenate((out6,out7),dim=-1)))),p=0.1)\n",
    "            out = self.DNN9(torch.concatenate((out7,out8),dim=-1))\n",
    "            if self.recording:\n",
    "                self.record = {'last_token':last_token,'out1':out1,'out2':out2,'out3':out3,'out4':out4,'out5':out5,'out6':out6,'out7':out7,'out8':out8,'out9':out}\n",
    "        else:\n",
    "            last_token = out[:,-1,:]\n",
    "            first_token = out[:,0,:]\n",
    "            out = F.dropout(F.relu(self.bn1(self.DNN1(torch.concatenate((first_token,last_token),dims=-1)))),p=0.1)\n",
    "            out = F.dropout(F.relu(self.bn2(self.DNN2(out))),p=0.1)\n",
    "            out = F.dropout(F.relu(self.bn3(self.DNN3(out))),p=0.1)\n",
    "            out = (self.DNN4(out))\n",
    "        out = F.sigmoid(out)\n",
    "        out = torch.reshape(out,(-1,self.out))\n",
    "        if self.recording:\n",
    "            self.record['out'] = out\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidalCNN(nn.Module):\n",
    "    def __init__(self,num_heads=1,output_size=4,kernel_size=2):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Takes a batch or an unbatched input and assimilated information from the surrounding bases, until only one vector remians\n",
    "        input shape = (N,1,82) 80 LENGTH OF SEQUENCE + 1 START CODE +1 END CODE\n",
    "        '''\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.cnn_list = []\n",
    "        for i in range(81):\n",
    "            if i==0:\n",
    "                self.cnn_list.append(nn.Conv1d(4,num_heads,kernel_size=kernel_size,padding=0))\n",
    "            elif i==80:\n",
    "                self.cnn_list.append(nn.Conv1d(num_heads,output_size,kernel_size=kernel_size,padding=0))\n",
    "            else:\n",
    "                self.cnn_list.append(nn.Conv1d(num_heads,num_heads,kernel_size=kernel_size,padding=0))\n",
    "        for i,c in enumerate(self.cnn_list):\n",
    "            self.add_module(name=str(c)[:6]+'_'+str(i),module=c)\n",
    "    def forward(self,x):\n",
    "        # the shape of x must be N,1,82\n",
    "        for c in self.cnn_list:\n",
    "            x = F.relu(c(F.pad(x,(self.kernel_size-2,0))))\n",
    "        return torch.reshape(x,(-1,self.output_size))\n",
    "class PyramidalClassifier(nn.Module):\n",
    "    def __init__(self,num_heads,output_size,kernel_size):\n",
    "        super().__init__()\n",
    "        self.PyCNN = PyramidalCNN(num_heads=num_heads,output_size=output_size,kernel_size=kernel_size)\n",
    "        self.DNN = nn.Linear(output_size,1)\n",
    "    def forward(self,x):\n",
    "        x = self.PyCNN(x)\n",
    "        x = F.sigmoid(self.DNN(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNNBreathing(nn.Module):\n",
    "    def __init__(self,num_heads=1,output_size=4,kernel_size=2,total=5):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Takes a batch or an unbatched input and assimilated information from the surrounding bases, until only one vector remians\n",
    "        input shape = (N,1,82) 80 LENGTH OF SEQUENCE + 1 START CODE +1 END CODE\n",
    "        '''\n",
    "        self.output_size = output_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.cnn_list = []\n",
    "        for i in range(total):\n",
    "            if i==0:\n",
    "                self.cnn_list.append(nn.Conv1d(4,num_heads,kernel_size=kernel_size))\n",
    "            elif i==total-1:\n",
    "                self.cnn_list.append(nn.ConvTranspose1d(num_heads,num_heads,kernel_size=kernel_size))\n",
    "                self.cnn_list.append(nn.Conv1d(num_heads,output_size,kernel_size=kernel_size))\n",
    "                self.cnn_list.append(nn.ConvTranspose1d(output_size,output_size,kernel_size=kernel_size))\n",
    "            else:\n",
    "                self.cnn_list.append(nn.ConvTranspose1d(num_heads,num_heads,kernel_size=kernel_size))\n",
    "                self.cnn_list.append(nn.Conv1d(num_heads,num_heads,kernel_size=kernel_size))\n",
    "        for i,c in enumerate(self.cnn_list):\n",
    "            self.add_module(name=str(c)[:6]+'_'+str(i),module=c)\n",
    "    def forward(self,x):\n",
    "        # the shape of x must be N,1,82\n",
    "        for c in self.cnn_list:\n",
    "            # print(c)\n",
    "            x = F.relu(c(x))\n",
    "        #     print(x.shape)\n",
    "        # print(x.shape)\n",
    "        return torch.reshape(x,(-1,self.output_size))\n",
    "class NormalCNNClassifier(nn.Module):\n",
    "    def __init__(self,num_heads=4,output_size=4,kernel_size=2,total=4):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.PyCNN = CNNBreathing(num_heads=num_heads,output_size=output_size,kernel_size=kernel_size,total=total)\n",
    "        self.DNN = nn.Sequential(nn.Linear(output_size*82,512),nn.BatchNorm1d(512),nn.ReLU(),nn.Dropout(p=0.1),nn.Linear(512,1))\n",
    "        \n",
    "        # self.DNN = nn.Sequential(nn.Linear(output_size*82,512),nn.BatchNorm1d(512),nn.ReLU(),nn.Dropout(p=0.1),nn.Linear(512,512),nn.BatchNorm1d(512),nn.ReLU(),nn.Dropout(p=0.1),nn.Linear(512,512),nn.BatchNorm1d(512),nn.ReLU(),nn.Dropout(p=0.1),nn.Linear(512,1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.PyCNN(x)\n",
    "        x = F.sigmoid(self.DNN(torch.reshape(x,(-1,self.output_size*(82)))))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2740985/644165451.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_LSTM = torch.load('/storage/madhu/deep/CFG/Project/models/model_LSTMModel_0.9070017773549953.pt')\n",
      "/tmp/ipykernel_2740985/644165451.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_LSTM_skip = torch.load('/storage/madhu/deep/CFG/Project/models/model_LSTMModelWithSkip_0.8882519338123013.pt')\n",
      "/tmp/ipykernel_2740985/644165451.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_CNN_Breather = torch.load('/storage/madhu/deep/CFG/Project/models/model_NormalCNNClassifier_0.8619170400781034.pt')\n",
      "/tmp/ipykernel_2740985/644165451.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_pyramidal = torch.load('/storage/madhu/deep/CFG/Project/models/model_PyramidalClassifier_0.497459133351691.pt')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_LSTM = torch.load('/storage/madhu/deep/CFG/Project/models/model_LSTMModel_0.9070017773549953.pt')\n",
    "model_LSTM_skip = torch.load('/storage/madhu/deep/CFG/Project/models/model_LSTMModelWithSkip_0.8882519338123013.pt')\n",
    "model_CNN_Breather = torch.load('/storage/madhu/deep/CFG/Project/models/model_NormalCNNClassifier_0.8619170400781034.pt')\n",
    "model_pyramidal = torch.load('/storage/madhu/deep/CFG/Project/models/model_PyramidalClassifier_0.497459133351691.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    model_LSTM.eval()\n",
    "    model_LSTM_skip.eval()\n",
    "    model_CNN_Breather.eval()\n",
    "    model_pyramidal.eval()\n",
    "    # print(test_data)\n",
    "    y_pred_LSTM = model_LSTM(test_data).detach().cpu().numpy()\n",
    "    y_pred_LSTM_skip = model_LSTM_skip(test_data).detach().cpu().numpy()\n",
    "    y_pred_CNN_Breather = model_CNN_Breather(test_data_CNN).detach().cpu().numpy()\n",
    "    y_pred_pyramidal = model_pyramidal(test_data_CNN).detach().cpu().numpy()\n",
    "    print(y_pred_LSTM.shape)\n",
    "    for i in range(len(y_pred_LSTM)):\n",
    "        with open('./results/LSTM_OUT.txt','a') as f:\n",
    "            f.write(f'{y_pred_LSTM[i][0]},\\n')\n",
    "        with open('./results/LSTM_SKIP_OUT.txt','a') as f:\n",
    "            f.write(f'{y_pred_LSTM_skip[i][0]},\\n')\n",
    "        with open('./results/CNN_Breather_OUT.txt','a') as f:\n",
    "            f.write(f'{y_pred_CNN_Breather[i][0]},\\n')\n",
    "        with open('./results/pyramidal_OUT.txt','a') as f:\n",
    "            f.write(f'{y_pred_pyramidal[i][0]},\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
